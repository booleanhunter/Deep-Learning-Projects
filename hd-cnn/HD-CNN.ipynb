{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hierarchical implementation for multi-label classifications\n",
    "\n",
    "The idea is to take advantage of the underlying hierarchies/categories that a number of objects (n) belong to, and classify them accurately, rather than having a flat n number of classes. The goal is to see if the performance of the classifier improves in any significant way.\n",
    "\n",
    "**Why introduce hierarchies?**\n",
    "\n",
    "Say for example, I had to categorize between 4 different objects - cat, dog, house, table.\n",
    "Cat and dog are related (they are living beings), whereas house and table are inanimate objects. \n",
    "In terms of how well my classifier performs when clasifying between these 4 objects - I would be more forgiving if my classifier mis-classifies a cat as a dog - atleast my classifier still understands that the cat is a living being! But I wouldn't like it if it misclassifies a table for a dog.\n",
    "\n",
    "**Any suggestions and advice on how to go about implementing this will be greatly welcome**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Here are some research papers I read for reference\n",
    "\n",
    "* https://arxiv.org/abs/1410.0736\n",
    "* https://arxiv.org/abs/1709.09890\n",
    "\n",
    "### Other references from the internet\n",
    "* https://keras.io/getting-started/functional-api-guide/#getting-started-with-the-keras-functional-api\n",
    "* https://machinelearningmastery.com/keras-functional-api-deep-learning/\n",
    "* https://github.com/ankonzoid/Google-QuickDraw/blob/master/QuickDraw_noisy_classifier.py\n",
    "* https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Architecture of HD-CNN for hierarchical classification looks like this:\n",
    "![](./models/HDD-CNN_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Images to be used for training are Numpy Bitmap files, taken from here: https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/numpy_bitmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Importing necessary libraries:\n",
    "\n",
    "I'm using Keras for experimenting with and building our CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, scipy.misc, pylab, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D \n",
    "from keras.models import load_model  # save keras model\n",
    "import pydot\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.merge import concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize some variables for training purposes\n",
    "\n",
    "#directory where the image files are present. Files are of the format .npy\n",
    "data_dir = \"./data\"\n",
    "\n",
    "#mention file names\n",
    "file_names = [\"house\", \"table\", \"cat\", \"dog\"]\n",
    "\n",
    "#Mention higher level of classes, in order.\n",
    "#0 for nonliving, 1 for living\n",
    "coarse_classes = [0, 0, 1, 1] \n",
    "\n",
    "#Mention lower level of classes (finer classes)\n",
    "# 0 for house, 1 for table, 2 for cat, 3 for dog\n",
    "fine_classes = [0, 1, 2, 3]\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 500\n",
    "\n",
    "xpixels = 28  # set x pixel numbers for query/training/test examples\n",
    "ypixels = 28  # set y pixel numbers for query/training/test examples\n",
    "input_shape = (ypixels, xpixels, 1)  # our data format for the input layer of our NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# converts image list to a normalized numpy array which will be used for training our CNN\n",
    "def convert_img2norm(img_list, ypixels, xpixels):\n",
    "    norm_list = img_list.copy()\n",
    "    norm_list = norm_list.astype('float32') / 255\n",
    "    norm_list = np.reshape(norm_list, (len(norm_list), ypixels, xpixels, 1))\n",
    "    return norm_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Takes in  file names, coarse classes and fine classes as input and returns output\n",
    "\n",
    "Input:\n",
    "Give the function how many training & testing samples you need\n",
    "\n",
    "Output:\n",
    "x_train, x_test\n",
    "y_train, y_test for coarse classes\n",
    "y_train_fine, y_test_fine for finer classes\n",
    "'''\n",
    "def preprocess_data(data_dir, file_names, coarse_classes, fine_classes, n_training_samples, n_testing_samples):\n",
    "    category_filenames = []\n",
    "    \n",
    "    for catname in file_names:\n",
    "        filename = os.path.join(data_dir, catname + \".npy\")\n",
    "        category_filenames.append(filename)\n",
    "        training_samples = []\n",
    "\n",
    "    n_categories = len(list(set(coarse_classes))) # number of classes\n",
    "    x_train = []\n",
    "    y_train_coarse = []; y_train_fine = []\n",
    "    \n",
    "    x_test = []\n",
    "    y_test_coarse = []; y_test_fine = []\n",
    "\n",
    "    for i_filename, filename in enumerate(file_names):\n",
    "        i_category_coarse = coarse_classes[i_filename] #respective coarse class\n",
    "        i_category_fine = fine_classes[i_filename] #respective fine class\n",
    "        \n",
    "        #load the input files\n",
    "        data = np.load(category_filenames[i_filename])\n",
    "\n",
    "        n_data = len(data)\n",
    "        print(\"[%d/%d] Reading filename index %d: '%s' under coarse category '%s' and fine category '%s' (%d images: take %d training samples, take %d testing samples)\" %\n",
    "              (i_filename+1, len(file_names), i_filename, filename, i_category_coarse, i_category_fine, n_data, n_training_samples, n_testing_samples))\n",
    "\n",
    "        #Split into training and testing sets\n",
    "        for j, data_j in enumerate(data):\n",
    "            img = np.array(data_j).reshape((ypixels, xpixels))\n",
    "            if j < n_training_samples:\n",
    "                # append to training set\n",
    "                x_train.append(img)\n",
    "                y_train_coarse.append(i_category_coarse) \n",
    "                y_train_fine.append(i_category_fine)   \n",
    "            elif j - n_training_samples < n_testing_samples:\n",
    "                # append to test set\n",
    "                x_test.append(img)\n",
    "                y_test_coarse.append(i_category_coarse)\n",
    "                y_test_fine.append(i_category_fine)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    x_train = np.array(x_train) \n",
    "    y_train_coarse = np.array(y_train_coarse); y_train_fine = np.array(y_train_fine)\n",
    "    \n",
    "    x_test = np.array(x_test) \n",
    "    y_test_coarse = np.array(y_test_coarse); y_test_fine = np.array(y_test_fine)\n",
    "\n",
    "    # Convert our greyscaled image data sets to have values [0,1] and reshape to form (n, ypixels, xpixels, 1)\n",
    "    x_train = convert_img2norm(x_train, ypixels, xpixels)\n",
    "    x_test = convert_img2norm(x_test, ypixels, xpixels)\n",
    "    \n",
    "    return x_train, y_train_coarse, y_train_fine, x_test, y_test_coarse, y_test_fine, n_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Use the *preprocess_data* function to divide the data into training and testing samples for both coarse and fine categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/4] Reading filename index 0: 'house' under coarse category '0' and fine category '0' (135420 images: take 50000 training samples, take 10000 testing samples)\n",
      "[2/4] Reading filename index 1: 'table' under coarse category '0' and fine category '1' (128021 images: take 50000 training samples, take 10000 testing samples)\n",
      "[3/4] Reading filename index 2: 'cat' under coarse category '1' and fine category '2' (123202 images: take 50000 training samples, take 10000 testing samples)\n",
      "[4/4] Reading filename index 3: 'dog' under coarse category '1' and fine category '3' (152159 images: take 50000 training samples, take 10000 testing samples)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train_coarse, y_train_fine, x_test, y_test_coarse, y_test_fine, n_categories = preprocess_data(data_dir, file_names, coarse_classes, fine_classes, 50000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Just returns the architecture of the simpler working model for comparison purposes\n",
    "def simple_sequential_model(model_path):\n",
    "    # Build our CNN mode layer-by-layer\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    cnn.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    cnn.add(Dropout(0.25))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(128, activation='relu'))\n",
    "    cnn.add(Dropout(0.5))\n",
    "    cnn.add(Dense(n_categories, activation='softmax')) \n",
    "\n",
    "    cnn.summary()\n",
    "\n",
    "    # Set our optimizer and loss function (similar settings to our CAE approach)\n",
    "    cnn.compile(loss = keras.losses.sparse_categorical_crossentropy,\n",
    "                optimizer = keras.optimizers.Adadelta(),\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "    plot_model(cnn, to_file = model_path+'.png')\n",
    "    return cnn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Trying out a simple hierarchical architecture first that can perform multi label classification:\n",
    "Please have a look at *working_model.png*. Performing training and testing on this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Trying out an architecture that can perform multi-label classification\n",
    "def build_cnn(model_path):\n",
    "    #first build a shared layer\n",
    "    input_1 = Input(shape=input_shape)\n",
    "    \n",
    "    #build coarse component layer\n",
    "    conv_1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(input_1)\n",
    "    conv_2 = Conv2D(64, kernel_size = (3,3), activation=\"relu\")(conv_1)\n",
    "    pool_1 = MaxPooling2D(pool_size=(2,2))(conv_2)\n",
    "    \n",
    "    flatten_1 = Flatten()(pool_1)\n",
    "    #coarse output prediction\n",
    "    output_coarse = Dense(2, activation=\"softmax\")(flatten_1)\n",
    "    \n",
    "    #fine feature component layer\n",
    "    dropout_1 = Dropout(0.25)(pool_1)\n",
    "    conv_3 = Conv2D(64, kernel_size = (3,3), activation=\"relu\")(dropout_1)\n",
    "    flatten = Flatten()(conv_3)\n",
    "    dense_1 = Dense(128, activation=\"relu\")(flatten)\n",
    "    dropout_2 = Dropout(0.5)(dense_1)\n",
    "    \n",
    "    #this will give us the fine category predictions\n",
    "    output_fine = Dense(4, activation=\"softmax\")(dropout_2)\n",
    "    \n",
    "    model = Model(inputs=input_1, outputs=[output_coarse, output_fine])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    # Set our optimizer and loss function\n",
    "    model.compile(loss = keras.losses.sparse_categorical_crossentropy,\n",
    "                optimizer = keras.optimizers.Adadelta(),\n",
    "                metrics = ['accuracy'])   \n",
    "    \n",
    "    plot_model(model, to_file=model_path+'.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 26, 26, 32)   320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 24, 24, 64)   18496       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 12, 12, 64)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 12, 12, 64)   0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 10, 10, 64)   36928       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 6400)         0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          819328      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 9216)         0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            18434       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            516         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 894,022\n",
      "Trainable params: 894,022\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mymodel = build_cnn(\"./models/working_model_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Current working model architecture - simple hierarchical one (*/models/working_model_1.png*): \n",
    "![](./models/working_model_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Function for training and saving our model\n",
    "def train_validate_save(model, model_path, x_train, y_train_coarse, y_train_fine, x_test, y_test_coarse, y_test_fine): \n",
    "    model.fit(x_train, [y_train_coarse, y_train_fine],\n",
    "            batch_size = batch_size,\n",
    "            epochs = n_epochs,\n",
    "            verbose = 1,\n",
    "            validation_data = (x_test, [y_test_coarse, y_test_fine]))\n",
    "\n",
    "    # cnn trained CNN model\n",
    "    model.save(model_path)  # creates a HDF5 file\n",
    "\n",
    "    # Evaluate our model test loss/accuracy\n",
    "    score = model.evaluate(x_test, [y_test_coarse, y_test_fine], verbose=1)\n",
    "    print(\"CNN Classification test performance:\")\n",
    "    print(score)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 40000 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 351s 2ms/step - loss: 0.1763 - dense_1_loss: 0.0404 - dense_3_loss: 0.1359 - dense_1_acc: 0.9872 - dense_3_acc: 0.9475 - val_loss: 0.2097 - val_dense_1_loss: 0.0483 - val_dense_3_loss: 0.1614 - val_dense_1_acc: 0.9844 - val_dense_3_acc: 0.9386\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 340s 2ms/step - loss: 0.1702 - dense_1_loss: 0.0386 - dense_3_loss: 0.1316 - dense_1_acc: 0.9876 - dense_3_acc: 0.9488 - val_loss: 0.2075 - val_dense_1_loss: 0.0475 - val_dense_3_loss: 0.1601 - val_dense_1_acc: 0.9851 - val_dense_3_acc: 0.9411\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 341s 2ms/step - loss: 0.1644 - dense_1_loss: 0.0373 - dense_3_loss: 0.1271 - dense_1_acc: 0.9881 - dense_3_acc: 0.9507 - val_loss: 0.2084 - val_dense_1_loss: 0.0482 - val_dense_3_loss: 0.1602 - val_dense_1_acc: 0.9842 - val_dense_3_acc: 0.9406\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 339s 2ms/step - loss: 0.1584 - dense_1_loss: 0.0357 - dense_3_loss: 0.1226 - dense_1_acc: 0.9886 - dense_3_acc: 0.9524 - val_loss: 0.2111 - val_dense_1_loss: 0.0473 - val_dense_3_loss: 0.1638 - val_dense_1_acc: 0.9846 - val_dense_3_acc: 0.9410\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 339s 2ms/step - loss: 0.1525 - dense_1_loss: 0.0345 - dense_3_loss: 0.1180 - dense_1_acc: 0.9889 - dense_3_acc: 0.9538 - val_loss: 0.2036 - val_dense_1_loss: 0.0464 - val_dense_3_loss: 0.1572 - val_dense_1_acc: 0.9862 - val_dense_3_acc: 0.9416\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 333s 2ms/step - loss: 0.1469 - dense_1_loss: 0.0331 - dense_3_loss: 0.1138 - dense_1_acc: 0.9895 - dense_3_acc: 0.9549 - val_loss: 0.2100 - val_dense_1_loss: 0.0470 - val_dense_3_loss: 0.1630 - val_dense_1_acc: 0.9858 - val_dense_3_acc: 0.9414\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 333s 2ms/step - loss: 0.1415 - dense_1_loss: 0.0320 - dense_3_loss: 0.1095 - dense_1_acc: 0.9897 - dense_3_acc: 0.9571 - val_loss: 0.2094 - val_dense_1_loss: 0.0458 - val_dense_3_loss: 0.1635 - val_dense_1_acc: 0.9861 - val_dense_3_acc: 0.9406\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 334s 2ms/step - loss: 0.1374 - dense_1_loss: 0.0315 - dense_3_loss: 0.1060 - dense_1_acc: 0.9898 - dense_3_acc: 0.9584 - val_loss: 0.2198 - val_dense_1_loss: 0.0465 - val_dense_3_loss: 0.1733 - val_dense_1_acc: 0.9855 - val_dense_3_acc: 0.9408\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 338s 2ms/step - loss: 0.1334 - dense_1_loss: 0.0300 - dense_3_loss: 0.1034 - dense_1_acc: 0.9905 - dense_3_acc: 0.9593 - val_loss: 0.2144 - val_dense_1_loss: 0.0486 - val_dense_3_loss: 0.1658 - val_dense_1_acc: 0.9838 - val_dense_3_acc: 0.9410\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 338s 2ms/step - loss: 0.1295 - dense_1_loss: 0.0294 - dense_3_loss: 0.1001 - dense_1_acc: 0.9906 - dense_3_acc: 0.9604 - val_loss: 0.2134 - val_dense_1_loss: 0.0459 - val_dense_3_loss: 0.1675 - val_dense_1_acc: 0.9852 - val_dense_3_acc: 0.9410\n",
      "40000/40000 [==============================] - 16s 391us/step\n",
      "CNN Classification test performance:\n",
      "[0.213365438794301, 0.04591087463991426, 0.16745456401039277, 0.9852, 0.94095]\n"
     ]
    }
   ],
   "source": [
    "classifier_path = './models/working_model_1.h5'\n",
    "\n",
    "if os.path.isfile(classifier_path):\n",
    "    classifier = load_model(classifier_path)  # load saved model\n",
    "    classifier.summary() \n",
    "else:\n",
    "    # Build our CNN layer-by-layer\n",
    "    classifier = train_validate_save(mymodel, classifier_path, x_train, y_train_coarse, y_train_fine, x_test, y_test_coarse, y_test_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 18s 458us/step\n",
      "[0.213365438794301, 0.04591087463991426, 0.16745456401039277, 0.9852, 0.94095]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our model test loss/accuracy\n",
    "score = classifier.evaluate(x_test, [y_test_coarse, y_test_fine], verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Function to test the model out. \n",
    "def test_model(model, img_path):\n",
    "    from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "\n",
    "    # loads RGB image as PIL.Image.Image type and converts into greyscale\n",
    "    img = img_to_array(load_img(img_path, grayscale=True, target_size=(28, 28)))\n",
    "    \n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (28, 28, 1)\n",
    "    img = np.array(img)\n",
    "    \n",
    "    #print(img.shape) #print shape of image\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 28, 28, 1) and return 4D tensor\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    prediction = model.predict(img)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Observations:\n",
    "\n",
    "* Trained the classifier on 40,000 samples from each category and validated against 10,000 from each category (total of 1,60,000 training samples and 40,000 testing samples). \n",
    "* After 10 epochs, the validation score appears to be 0.9829 for the coarse categories (living, non-living) and 0.9345 for the finer categories (house, table, cat, dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experimental model, which I want to implement like in the paper https://arxiv.org/abs/1410.0736:\n",
    "Need some more data preprocessing before I can test it.\n",
    "\n",
    "Please have a look at the architecture image titled *experimental_model.png*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#experimental model architecture\n",
    "def build_cnn_experimental(model_path):\n",
    "    #first build a shared layer\n",
    "    input_1 = Input(shape=input_shape)\n",
    "    conv_1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(input_1)\n",
    "    pool_1 = MaxPooling2D(pool_size=(1, 1))(conv_1)\n",
    "    dense_1 = Dense(2, activation=\"softmax\")(pool_1)\n",
    "    \n",
    "    #build coarse component layer\n",
    "    conv_2 = Conv2D(32, kernel_size = (1,1), activation=\"relu\")(dense_1)\n",
    "    pool_2 = MaxPooling2D(pool_size=(1,1))(conv_2)\n",
    "    \n",
    "    #coarse output prediction\n",
    "    output_coarse = Dense(2, activation=\"softmax\")(pool_2)\n",
    "    \n",
    "    #fine features\n",
    "    merge = concatenate([dense_1, output_coarse])\n",
    "    \n",
    "    #fine feature 1\n",
    "    conv_3 = Conv2D(32, kernel_size=(1,1), activation=\"relu\")(merge)\n",
    "    pool_3 = MaxPooling2D(pool_size=(2, 2))(conv_3)\n",
    "    flat_1 = Flatten()(pool_3)\n",
    "    output_fine_1 = Dense(4, activation='softmax')(flat_1)\n",
    "    \n",
    "    conv_4 = Conv2D(32, kernel_size=(1,1), activation=\"relu\")(merge)\n",
    "    pool_4 = MaxPooling2D(pool_size=(2, 2))(conv_4)\n",
    "    flat_2 = Flatten()(pool_4)\n",
    "    output_fine_2 = Dense(4, activation='softmax')(flat_2)\n",
    "    \n",
    "    model = Model(inputs=input_1, outputs=[output_coarse, output_fine_1, output_fine_2])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    # Set our optimizer and loss function\n",
    "    model.compile(loss = keras.losses.sparse_categorical_crossentropy,\n",
    "                optimizer = keras.optimizers.Adadelta(),\n",
    "                metrics = ['accuracy'])   \n",
    "    \n",
    "    plot_model(model, to_file=model_path+'.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 26, 26, 32)   320         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 26, 26, 32)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 26, 26, 2)    66          max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 26, 26, 32)   96          dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 26, 26, 32)   0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 26, 26, 2)    66          max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 26, 26, 4)    0           dense_15[0][0]                   \n",
      "                                                                 dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 26, 26, 32)   160         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 26, 26, 32)   160         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 13, 13, 32)   0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 13, 13, 32)   0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 5408)         0           max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 5408)         0           max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 4)            21636       flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 4)            21636       flatten_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 44,140\n",
      "Trainable params: 44,140\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "experimental_model = build_cnn_experimental(\"./models/experimental_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Architecture of the experimental model looks like this (*/models/experimental_model.png*):\n",
    "![](./models/experimental_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The training approach:\n",
    "\n",
    "* First, train the coarse category component - a regular CNN with few convolutional layers\n",
    "* This will also be used as a shared layer for initializing the fine category components\n",
    "* This component will also give an output for the coarse, higher level categories (non-living or living)\n",
    "\n",
    "Next, for training upon the fine categories:\n",
    "* Each of the fine category components are separate CNNs\n",
    "* For both the components, we will initialize the initial rear layers by copying the weights from the shared layer of the coarse component. The weights are kept fixed/frozen for the initial layers\n",
    "* Once initialized, we will train each component by only using the images from their respective coarse categories.\n",
    "\n",
    "  For fine category component 1, we will train it using only examples of non-living objects, example (house, table)\n",
    "  \n",
    "  For fine category component 2, we will train it using only examples of living objects, example (cat, dog)\n",
    "  \n",
    "  Each fine tune component will spit out the second set of outputs for finer categories\n",
    "* Once all the parameters are trained, we will fine tune the entire HD-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
